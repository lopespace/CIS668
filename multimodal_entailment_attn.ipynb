{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multimodal_entailment_attn.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVBuqCa2P5hM"
      },
      "source": [
        "## Setup and data collection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3L2Uas2whxM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fe411e0-77d7-4362-ec4e-1bbb61d8bb32"
      },
      "source": [
        "#1 prepare for data\n",
        "!pip install -q tensorflow_text"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 5.8 MB 20.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 588.3 MB 6.1 kB/s \n",
            "\u001b[K     |████████████████████████████████| 439 kB 65.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 55.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 57.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqjVLu_uh4wJ"
      },
      "source": [
        "# 2 prepare for data\n",
        "!wget -q https://github.com/sayakpaul/Multimodal-Entailment-Baseline/releases/download/v1.0.0/tweet_images.tar.gz\n",
        "!tar xf tweet_images.tar.gz"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58cNoAt5P8Yj"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYqTcaAnh5_N"
      },
      "source": [
        "# 3 import the require library\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from tensorflow import keras\n",
        "\n",
        "tf.random.set_seed(13)\n",
        "np.random.seed(13)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT2XrM3YQBpu"
      },
      "source": [
        "## Data reading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiVz7oyQh9T1",
        "outputId": "7f4a87f6-926a-42d1-f8b3-c0c90dceead6"
      },
      "source": [
        "# 4 convert origin data to the data program can read\n",
        "train_df = pd.read_csv(\"https://github.com/sayakpaul/Multimodal-Entailment-Baseline/raw/main/csvs/train_df.csv\")\n",
        "val_df = pd.read_csv(\"https://github.com/sayakpaul/Multimodal-Entailment-Baseline/raw/main/csvs/val_df.csv\")\n",
        "test_df = pd.read_csv(\"https://github.com/sayakpaul/Multimodal-Entailment-Baseline/raw/main/csvs/test_df.csv\")\n",
        "\n",
        "print(f\"Total training examples: {len(train_df)}\")\n",
        "print(f\"Total validation examples: {len(val_df)}\")\n",
        "print(f\"Total test examples: {len(test_df)}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training examples: 1197\n",
            "Total validation examples: 63\n",
            "Total test examples: 140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbrSSXttQDUQ"
      },
      "source": [
        "## Data input pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uGoijH4iIL2"
      },
      "source": [
        "# 5\n",
        "# Define TF Hub paths to the BERT encoder and its preprocessor.\n",
        "bert_model_path = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1\"\n",
        "bert_preprocess_path = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4-uBN1yiO-W"
      },
      "source": [
        "# 6 create a function to build model\n",
        "\n",
        "# Reference:\n",
        "# https://www.tensorflow.org/text/tutorials/bert_glue\n",
        "\n",
        "def make_bert_preprocess_model(sentence_features, seq_length=128):\n",
        "  \"\"\"Returns Model mapping string features to BERT inputs.\n",
        "\n",
        "  Args:\n",
        "    sentence_features: a list with the names of string-valued features.\n",
        "    seq_length: an integer that defines the sequence length of BERT inputs.\n",
        "\n",
        "  Returns:\n",
        "    A Keras Model that can be called on a list or dict of string Tensors\n",
        "    (with the order or names, resp., given by sentence_features) and\n",
        "    returns a dict of tensors for input to BERT.\n",
        "  \"\"\"\n",
        "\n",
        "  input_segments = [\n",
        "      tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n",
        "      for ft in sentence_features]\n",
        "\n",
        "  # Tokenize the text to word pieces.\n",
        "  bert_preprocess = hub.load(bert_preprocess_path)\n",
        "  tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')\n",
        "  segments = [tokenizer(s) for s in input_segments]\n",
        "\n",
        "  # Optional: Trim segments in a smart way to fit seq_length.\n",
        "  # Simple cases (like this example) can skip this step and let\n",
        "  # the next step apply a default truncation to approximately equal lengths.\n",
        "  truncated_segments = segments\n",
        "\n",
        "  # Pack inputs. The details (start/end token ids, dict of output tensors)\n",
        "  # are model-dependent, so this gets loaded from the SavedModel.\n",
        "  packer = hub.KerasLayer(bert_preprocess.bert_pack_inputs,\n",
        "                          arguments=dict(seq_length=seq_length),\n",
        "                          name='packer')\n",
        "  model_inputs = packer(truncated_segments)\n",
        "  return keras.Model(input_segments, model_inputs)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRXSdrB6iQlN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bb0a210-e4b8-4fe3-9ae3-c6423bb1dfaf"
      },
      "source": [
        "# 7 build model\n",
        "bert_preprocess_model = make_bert_preprocess_model(['text_1', 'text_2'])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXM6fUohiS7s"
      },
      "source": [
        "# 8 create a function to convert to dataset\n",
        "\n",
        "# Reference:\n",
        "# https://keras.io/examples/structured_data/structured_data_classification_from_scratch/\n",
        "def dataframe_to_dataset(dataframe):\n",
        "    columns = [\"image_1_path\", \"image_2_path\", \"text_1\", \"text_2\", \"label_idx\"]\n",
        "    dataframe = dataframe[columns].copy()\n",
        "    labels = dataframe.pop(\"label_idx\")\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "    return ds"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXTZDw1iiVBu"
      },
      "source": [
        "# 9 create several functions to read data and process data\n",
        "\n",
        "resize = (128, 128)\n",
        "bert_input_features = ['input_word_ids', 'input_type_ids', 'input_mask']\n",
        "\n",
        "def read_resize(image_path):\n",
        "    extension = tf.strings.split(image_path)[-1]\n",
        "\n",
        "    image = tf.io.read_file(image_path)\n",
        "    if extension == b\"jpg\":\n",
        "        image = tf.image.decode_jpeg(image, 3)\n",
        "    else:\n",
        "        image = tf.image.decode_png(image, 3)\n",
        "    image = tf.image.resize(image, resize)\n",
        "    return image\n",
        "\n",
        "def preprocess_text(text_1, text_2):\n",
        "    text_1 = tf.convert_to_tensor([text_1])\n",
        "    text_2 = tf.convert_to_tensor([text_2])\n",
        "    output = bert_preprocess_model([text_1, text_2])\n",
        "    output = {feature: tf.squeeze(output[feature]) \n",
        "        for feature in bert_input_features}\n",
        "    return output\n",
        "\n",
        "def preprocess(sample):\n",
        "    image_1 = read_resize(sample[\"image_1_path\"])\n",
        "    image_2 = read_resize(sample[\"image_2_path\"])\n",
        "    text = preprocess_text(sample[\"text_1\"], sample[\"text_2\"])\n",
        "    return {\"image_1\": image_1, \"image_2\": image_2, \"text\": text}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlIZB3tTiWxx"
      },
      "source": [
        "# 10 create function to process data\n",
        "\n",
        "batch_size = 32\n",
        "auto = tf.data.AUTOTUNE\n",
        "\n",
        "def prepare_dataset(df, training=True):\n",
        "    ds = dataframe_to_dataset(df)\n",
        "    if training:\n",
        "        ds = ds.shuffle(len(train_df))\n",
        "    ds = ds.map(lambda x, y: (preprocess(x), y))\n",
        "    ds = ds.batch(batch_size).prefetch(auto)\n",
        "    return ds"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diJgdkE0QLwl"
      },
      "source": [
        "## Final datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vpFT4pKiYls"
      },
      "source": [
        "# 11 process data\n",
        "\n",
        "train_ds = prepare_dataset(train_df)\n",
        "validation_ds = prepare_dataset(val_df, False)\n",
        "test_ds = prepare_dataset(test_df, False)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SqbwhJkiav1"
      },
      "source": [
        "# 12\n",
        "# Separate the train and test labels for later evaluation.\n",
        "def separate_labels(ds):\n",
        "    labels = []\n",
        "    for _, label in ds.unbatch():\n",
        "        labels.append(label)\n",
        "    labels = np.array(labels)\n",
        "    return labels\n",
        "\n",
        "train_labels = separate_labels(train_ds)\n",
        "test_labels = separate_labels(test_ds)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_Ng98GvQOKX"
      },
      "source": [
        "## Model utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hzsPLgNQPu0"
      },
      "source": [
        "`project_embeddings()`, `create_vision_encoder()`, and `create_text_encoder()` come from [here](https://keras.io/examples/nlp/nl_image_search/). \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbYj7lBO8mdo"
      },
      "source": [
        "# 13 emded project\n",
        "\n",
        "def project_embeddings(\n",
        "    embeddings, num_projection_layers, projection_dims, dropout_rate\n",
        "):\n",
        "    projected_embeddings = keras.layers.Dense(units=projection_dims)(embeddings)\n",
        "    for _ in range(num_projection_layers):\n",
        "        x = tf.nn.gelu(projected_embeddings)\n",
        "        x = keras.layers.Dense(projection_dims)(x)\n",
        "        x = keras.layers.Dropout(dropout_rate)(x)\n",
        "        x = keras.layers.Add()([projected_embeddings, x])\n",
        "        projected_embeddings = keras.layers.LayerNormalization()(x)\n",
        "    return projected_embeddings"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3OHyy8x86Is"
      },
      "source": [
        "# 14 create a function to encoder\n",
        "\n",
        "def create_vision_encoder(\n",
        "    num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
        "):\n",
        "    # Load the pre-trained ResNet50V2 model to be used as the base encoder.\n",
        "    resnet_v2 = keras.applications.ResNet50V2(\n",
        "        include_top=False, weights=\"imagenet\", pooling=\"avg\"\n",
        "    )\n",
        "    # Set the trainability of the base encoder.\n",
        "    for layer in resnet_v2.layers:\n",
        "        layer.trainable = trainable\n",
        "    \n",
        "    # Receive the images as inputs.\n",
        "    image_1 = keras.Input(shape=(128, 128, 3), name=\"image_1\")\n",
        "    image_2 = keras.Input(shape=(128, 128, 3), name=\"image_2\")\n",
        "    \n",
        "    # Preprocess the input image.\n",
        "    preprocessed_1 = keras.applications.resnet_v2.preprocess_input(image_1)\n",
        "    preprocessed_2 = keras.applications.resnet_v2.preprocess_input(image_2)\n",
        "    \n",
        "    # Generate the embeddings for the images using the resnet_v2 model\n",
        "    # concatenate them.\n",
        "    embeddings_1 = resnet_v2(preprocessed_1)\n",
        "    embeddings_2 = resnet_v2(preprocessed_2)\n",
        "    embeddings = keras.layers.Concatenate()([embeddings_1, embeddings_2])\n",
        "    \n",
        "    # Project the embeddings produced by the model.\n",
        "    outputs = project_embeddings(\n",
        "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
        "    )\n",
        "    # Create the vision encoder model.\n",
        "    return keras.Model([image_1, image_2], outputs, name=\"vision_encoder\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qKs0s-b-0F5"
      },
      "source": [
        "# 15 create a function to endocer text\n",
        "\n",
        "def create_text_encoder(\n",
        "    num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
        "):\n",
        "    # Load the pre-trained BERT model to be used as the base encoder.\n",
        "    bert = hub.KerasLayer(\n",
        "        bert_model_path,\n",
        "        name=\"bert\",\n",
        "    )\n",
        "    # Set the trainability of the base encoder.\n",
        "    bert.trainable = trainable\n",
        "    \n",
        "    # Receive the text as inputs.\n",
        "    bert_input_features = ['input_type_ids', 'input_mask', 'input_word_ids']\n",
        "    inputs = {\n",
        "        feature: keras.Input(shape=(128, ), dtype=tf.int32, name=feature)\n",
        "        for feature in bert_input_features\n",
        "    }\n",
        "    \n",
        "    # Generate embeddings for the preprocessed text using the BERT model.\n",
        "    embeddings = bert(inputs)[\"pooled_output\"]\n",
        "    \n",
        "    # Project the embeddings produced by the model.\n",
        "    outputs = project_embeddings(\n",
        "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
        "    )\n",
        "    # Create the text encoder model.\n",
        "    return keras.Model(inputs, outputs, name=\"text_encoder\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxNkEVGgicwc"
      },
      "source": [
        "# 16 create a function to create multimodal model\n",
        "\n",
        "def create_multimodal_model(num_projection_layers=1, projection_dims=256, dropout_rate=0.1, \n",
        "                     vision_trainable=False, text_trainable=False, attention=False):\n",
        "    # Receive the images as inputs.\n",
        "    image_1 = keras.Input(shape=(128, 128, 3), name=\"image_1\")\n",
        "    image_2 = keras.Input(shape=(128, 128, 3), name=\"image_2\")\n",
        "\n",
        "    # Receive the text as inputs.\n",
        "    bert_input_features = ['input_type_ids', 'input_mask', 'input_word_ids']\n",
        "    text_inputs = {\n",
        "        feature: keras.Input(shape=(128, ), dtype=tf.int32, name=feature)\n",
        "        for feature in bert_input_features\n",
        "    }\n",
        "\n",
        "    # Create the encoders.\n",
        "    vision_encoder = create_vision_encoder(num_projection_layers, projection_dims, dropout_rate, vision_trainable)\n",
        "    text_encoder = create_text_encoder(num_projection_layers, projection_dims, dropout_rate, text_trainable)\n",
        "\n",
        "    # Fetch the embedding projections.\n",
        "    vision_projections = vision_encoder([image_1, image_2])\n",
        "    text_projections = text_encoder(text_inputs)\n",
        "\n",
        "    # Cross-attention.\n",
        "    if attention:\n",
        "        query_value_attention_seq = keras.layers.Attention(use_scale=True, dropout=0.2)(\n",
        "            [vision_projections, text_projections]\n",
        "        )\n",
        "\n",
        "    # Concatenate the projections and pass through the classification layer.\n",
        "    concatenated = keras.layers.Concatenate()([vision_projections, text_projections])\n",
        "    if attention:\n",
        "        concatenated = keras.layers.Concatenate()([concatenated, query_value_attention_seq])\n",
        "    outputs = keras.layers.Dense(3, activation=\"softmax\")(concatenated)\n",
        "    return keras.Model([image_1, image_2, text_inputs], outputs)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DH1I_yxQcUO"
      },
      "source": [
        "## Model with cross-attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eefKljH9iuyt",
        "outputId": "3480a0ca-e774-4897-82f1-d3098ff31ab4"
      },
      "source": [
        "# 17 train model\n",
        "\n",
        "multimodal_model = create_multimodal_model(attention=True)\n",
        "multimodal_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=\"accuracy\")\n",
        "history = multimodal_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=validation_ds,\n",
        "    epochs=10\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94668760/94668760 [==============================] - 5s 0us/step\n",
            "Epoch 1/10\n",
            "38/38 [==============================] - 43s 623ms/step - loss: 0.8241 - accuracy: 0.8204 - val_loss: 0.6325 - val_accuracy: 0.8095\n",
            "Epoch 2/10\n",
            "38/38 [==============================] - 21s 538ms/step - loss: 0.4389 - accuracy: 0.8730 - val_loss: 0.5754 - val_accuracy: 0.8254\n",
            "Epoch 3/10\n",
            "38/38 [==============================] - 22s 569ms/step - loss: 0.3559 - accuracy: 0.8897 - val_loss: 0.5647 - val_accuracy: 0.8571\n",
            "Epoch 4/10\n",
            "38/38 [==============================] - 20s 522ms/step - loss: 0.2908 - accuracy: 0.9014 - val_loss: 0.6347 - val_accuracy: 0.8571\n",
            "Epoch 5/10\n",
            "38/38 [==============================] - 20s 535ms/step - loss: 0.1876 - accuracy: 0.9273 - val_loss: 0.8582 - val_accuracy: 0.8254\n",
            "Epoch 6/10\n",
            "38/38 [==============================] - 20s 521ms/step - loss: 0.1415 - accuracy: 0.9541 - val_loss: 0.9884 - val_accuracy: 0.8730\n",
            "Epoch 7/10\n",
            "38/38 [==============================] - 20s 522ms/step - loss: 0.0903 - accuracy: 0.9649 - val_loss: 1.0498 - val_accuracy: 0.8571\n",
            "Epoch 8/10\n",
            "38/38 [==============================] - 20s 518ms/step - loss: 0.0756 - accuracy: 0.9741 - val_loss: 1.0826 - val_accuracy: 0.8571\n",
            "Epoch 9/10\n",
            "38/38 [==============================] - 27s 712ms/step - loss: 0.0486 - accuracy: 0.9858 - val_loss: 1.2537 - val_accuracy: 0.8095\n",
            "Epoch 10/10\n",
            "38/38 [==============================] - 25s 646ms/step - loss: 0.0709 - accuracy: 0.9741 - val_loss: 1.2868 - val_accuracy: 0.8254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0nMGoEQi0y8",
        "outputId": "280cb6ad-98db-47fe-d99f-065f6e7b1ba4"
      },
      "source": [
        "# 18 show the accuracy on the test set\n",
        "\n",
        "_, acc = multimodal_model.evaluate(test_ds)\n",
        "print(f\"Accuracy on the test set: {round(acc * 100, 2)}%.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 4s 712ms/step - loss: 1.0095 - accuracy: 0.8357\n",
            "Accuracy on the test set: 83.57%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oO4sQ9VQegQ"
      },
      "source": [
        "## Model without cross-attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--dYfsnFjoNZ",
        "outputId": "09c708ef-985b-4ecf-f028-b609690ec047"
      },
      "source": [
        "# 19 train model without cross-attention\n",
        "\n",
        "multimodal_model = create_multimodal_model(attention=False)\n",
        "multimodal_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=\"accuracy\")\n",
        "history = multimodal_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=validation_ds,\n",
        "    epochs=10\n",
        ")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "38/38 [==============================] - 30s 538ms/step - loss: 1.0435 - accuracy: 0.8195 - val_loss: 0.5594 - val_accuracy: 0.8571\n",
            "Epoch 2/10\n",
            "38/38 [==============================] - 21s 549ms/step - loss: 0.4083 - accuracy: 0.8705 - val_loss: 0.6132 - val_accuracy: 0.8254\n",
            "Epoch 3/10\n",
            "38/38 [==============================] - 20s 528ms/step - loss: 0.3800 - accuracy: 0.8772 - val_loss: 0.6502 - val_accuracy: 0.8413\n",
            "Epoch 4/10\n",
            "38/38 [==============================] - 20s 518ms/step - loss: 0.3284 - accuracy: 0.8972 - val_loss: 0.6097 - val_accuracy: 0.8095\n",
            "Epoch 5/10\n",
            "38/38 [==============================] - 22s 573ms/step - loss: 0.2190 - accuracy: 0.9223 - val_loss: 0.9392 - val_accuracy: 0.8571\n",
            "Epoch 6/10\n",
            "38/38 [==============================] - 20s 522ms/step - loss: 0.1591 - accuracy: 0.9449 - val_loss: 0.9081 - val_accuracy: 0.8095\n",
            "Epoch 7/10\n",
            "38/38 [==============================] - 20s 517ms/step - loss: 0.0986 - accuracy: 0.9683 - val_loss: 1.1050 - val_accuracy: 0.8571\n",
            "Epoch 8/10\n",
            "38/38 [==============================] - 20s 517ms/step - loss: 0.0913 - accuracy: 0.9683 - val_loss: 1.0996 - val_accuracy: 0.8095\n",
            "Epoch 9/10\n",
            "38/38 [==============================] - 20s 514ms/step - loss: 0.0794 - accuracy: 0.9741 - val_loss: 1.2812 - val_accuracy: 0.8254\n",
            "Epoch 10/10\n",
            "38/38 [==============================] - 19s 509ms/step - loss: 0.0586 - accuracy: 0.9774 - val_loss: 1.3577 - val_accuracy: 0.8254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjI9GTSujzJk",
        "outputId": "514b0013-d80d-4e86-979a-727874b568fd"
      },
      "source": [
        "# 20 show the result\n",
        "\n",
        "_, acc = multimodal_model.evaluate(test_ds)\n",
        "print(f\"Accuracy on the test set: {round(acc * 100, 2)}%.\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 3s 410ms/step - loss: 1.1757 - accuracy: 0.7786\n",
            "Accuracy on the test set: 77.86%.\n"
          ]
        }
      ]
    }
  ]
}